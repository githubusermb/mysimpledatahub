# Project Folder Structure

## Overview

This document describes the organization of the mysimpledatahub project.

---

## Directory Layout

```
mysimpledatahub/
â”‚
â”œâ”€â”€ jobs/                                    # AWS Glue ETL Jobs
â”‚   â”œâ”€â”€ README.md                            # Job documentation
â”‚   â”œâ”€â”€ glue_csv_to_iceberg.py              # Data ingestion job
â”‚   â””â”€â”€ glue_create_normal_views.py    # View creation job
â”‚
â”œâ”€â”€ scripts/                                 # Helper & Utility Scripts
â”‚   â”œâ”€â”€ generate_sample_csv.py              # Generate test data
â”‚   â”œâ”€â”€ generate_sample_csv_mdrm.py         # Generate MDRM test data
â”‚   â”œâ”€â”€ setup_lakeformation_complete.py     # Lake Formation setup
â”‚   â”œâ”€â”€ grant_athena_user_permissions.py    # Grant permissions
â”‚   â”œâ”€â”€ register_table_with_lakeformation.py # Register tables
â”‚   â”œâ”€â”€ lambda_lakeformation_setup.py       # Lambda setup
â”‚   â”œâ”€â”€ test_views_in_spark.py              # Test views
â”‚   â”œâ”€â”€ diagnose_view.ps1                   # Diagnose views
â”‚   â”œâ”€â”€ drop_all_views.ps1                  # Drop all views
â”‚   â”œâ”€â”€ drop_all_views.sh                   # Drop all views (bash)
â”‚   â”œâ”€â”€ recreate_views.bat                  # Recreate views (Windows)
â”‚   â”œâ”€â”€ recreate_views.sh                   # Recreate views (bash)
â”‚   â”œâ”€â”€ test_views.bat                      # Test views (Windows)
â”‚   â”œâ”€â”€ test_views.sh                       # Test views (bash)
â”‚   â”œâ”€â”€ upload_and_process.sh               # Upload and process
â”‚   â”œâ”€â”€ test_pipeline_locally.sh            # Test pipeline
â”‚   â””â”€â”€ verify_view_format.ps1              # Verify view format
â”‚
â”œâ”€â”€ terraform/                               # Infrastructure as Code
â”‚   â”œâ”€â”€ main.tf                             # Main configuration
â”‚   â”œâ”€â”€ variables.tf                        # Variable definitions
â”‚   â”œâ”€â”€ outputs.tf                          # Output values
â”‚   â”œâ”€â”€ terraform.tfvars                    # Configuration values
â”‚   â”œâ”€â”€ lakeformation.tf                    # Lake Formation setup
â”‚   â”œâ”€â”€ views_dual_engine_job.tf            # Views job configuration
â”‚   â”œâ”€â”€ lambda_function.zip                 # Lambda deployment package
â”‚   â”œâ”€â”€ terraform.tfstate                   # Terraform state
â”‚   â”œâ”€â”€ terraform.tfstate.backup            # State backup
â”‚   â””â”€â”€ .terraform/                         # Terraform plugins
â”‚
â”œâ”€â”€ docs/                                    # Documentation
â”‚   â”œâ”€â”€ naming-conventions-best-practices.md # Naming guidelines
â”‚   â”œâ”€â”€ materialized-views-vs-iceberg-tables.md # View comparison
â”‚   â”œâ”€â”€ lake-formation-permissions.md       # Lake Formation guide
â”‚   â”œâ”€â”€ troubleshooting-lakeformation.md    # Troubleshooting
â”‚   â”œâ”€â”€ athena-permissions-setup.md         # Athena setup
â”‚   â”œâ”€â”€ automatic-lakeformation-setup.md    # Auto setup
â”‚   â”œâ”€â”€ fix-multi-dialect-view-error.md     # View fixes
â”‚   â”œâ”€â”€ querying-views-in-athena.md         # Query guide
â”‚   â”œâ”€â”€ running-scripts-in-aws.md           # AWS scripts
â”‚   â”œâ”€â”€ testing-views-in-spark.md           # Spark testing
â”‚   â””â”€â”€ verifying-views-in-catalog.md       # Catalog verification
â”‚
â”œâ”€â”€ data/                                    # Sample Data
â”‚   â””â”€â”€ ingest_ts=1770609249/
â”‚       â””â”€â”€ mdrm_data_1770609249.csv        # Sample MDRM data
â”‚
â”œâ”€â”€ mdrm/                                    # MDRM Output Files
â”‚   â”œâ”€â”€ output_fry15.txt                    # FRY-15 output
â”‚   â””â”€â”€ output_fry9c.txt                    # FRY-9C output
â”‚
â”œâ”€â”€ ARCHITECTURE.md                          # System architecture
â”œâ”€â”€ ENTITY-DIAGRAMS.md                       # Entity diagrams
â”œâ”€â”€ README.md                                # Main documentation
â”œâ”€â”€ QUICK-START.md                           # Quick start guide
â”œâ”€â”€ normal-SOLUTION.md                  # normal views
â”œâ”€â”€ MIGRATION-SUMMARY.md                     # Naming migration
â”œâ”€â”€ MIGRATION-JOBS-FOLDER.md                 # Jobs folder migration
â”œâ”€â”€ UPDATES-REGION-AND-VIEWS.md              # Recent updates
â”œâ”€â”€ BUGFIX-INGEST-TIMESTAMP.md               # Timestamp fix
â”œâ”€â”€ BUGFIX-VIEW-PREFIX-ARGUMENT.md           # View prefix fix
â”œâ”€â”€ FOLDER-STRUCTURE.md                      # This file
â”œâ”€â”€ prompt.md                                # Original prompt
â””â”€â”€ .gitignore                               # Git ignore rules
```

---

## Folder Purposes

### ğŸ“ jobs/

**Purpose**: AWS Glue ETL job scripts

**Contents**: Python scripts that run in AWS Glue environment

**Deployment**: Uploaded to S3 via Terraform, executed by Glue

**Key Files**:
- `glue_csv_to_iceberg.py` - Ingests CSV data into Iceberg tables
- `glue_create_normal_views.py` - Creates multi-dialect views

**When to Add Files Here**:
- New Glue ETL jobs
- Jobs that process data in Glue Spark
- Jobs that use awsglue libraries

### ğŸ“ scripts/

**Purpose**: Helper and utility scripts

**Contents**: Scripts for setup, testing, and maintenance

**Deployment**: Run locally or in Lambda, not deployed to Glue

**Key Files**:
- Setup scripts (Lake Formation, permissions)
- Test scripts (view testing, pipeline testing)
- Data generation scripts
- Diagnostic scripts

**When to Add Files Here**:
- Local utility scripts
- Setup/configuration scripts
- Testing scripts
- Lambda functions
- Shell scripts for automation

### ğŸ“ terraform/

**Purpose**: Infrastructure as Code

**Contents**: Terraform configuration files

**Deployment**: Applied via `terraform apply`

**Key Files**:
- `main.tf` - Main infrastructure (S3, Glue, IAM)
- `lakeformation.tf` - Lake Formation configuration
- `views_dual_engine_job.tf` - Views job configuration
- `variables.tf` - Variable definitions
- `terraform.tfvars` - Configuration values

**When to Add Files Here**:
- New AWS resources
- Infrastructure changes
- Job configurations
- IAM policies

### ğŸ“ docs/

**Purpose**: Documentation and guides

**Contents**: Markdown documentation files

**Key Files**:
- Best practices guides
- Troubleshooting guides
- Setup instructions
- Comparison documents

**When to Add Files Here**:
- New documentation
- How-to guides
- Architecture decisions
- Troubleshooting guides

### ğŸ“ data/

**Purpose**: Sample and test data

**Contents**: CSV files for testing

**When to Add Files Here**:
- Sample datasets
- Test data
- Reference data

### ğŸ“ mdrm/

**Purpose**: MDRM-specific output files

**Contents**: Output files from MDRM processing

**When to Add Files Here**:
- MDRM output files
- Series-specific data

---

## File Naming Conventions

### Python Scripts

```
Pattern: <purpose>_<action>.py

Examples:
âœ… glue_csv_to_iceberg.py
âœ… generate_sample_csv.py
âœ… setup_lakeformation_complete.py
âœ… grant_athena_user_permissions.py

âŒ script1.py
âŒ MyScript.py
âŒ test-script.py
```

### Shell Scripts

```
Pattern: <action>_<target>.<sh|bat|ps1>

Examples:
âœ… drop_all_views.sh
âœ… recreate_views.bat
âœ… test_pipeline_locally.sh
âœ… diagnose_view.ps1

âŒ script.sh
âŒ run.bat
```

### Documentation

```
Pattern: <TOPIC>-<SUBTOPIC>.md or <TOPIC>.md

Examples:
âœ… ARCHITECTURE.md
âœ… naming-conventions-best-practices.md
âœ… materialized-views-vs-iceberg-tables.md
âœ… BUGFIX-INGEST-TIMESTAMP.md

âŒ doc1.md
âŒ readme.txt
```

### Terraform Files

```
Pattern: <resource_type>.tf or main.tf

Examples:
âœ… main.tf
âœ… variables.tf
âœ… outputs.tf
âœ… lakeformation.tf
âœ… views_dual_engine_job.tf

âŒ config.tf
âŒ terraform.tf
```

---

## Adding New Files

### Adding a New Glue Job

1. Create script in `jobs/` folder:
   ```bash
   touch jobs/glue_new_job.py
   ```

2. Add Terraform configuration:
   ```hcl
   # In terraform/new_job.tf
   resource "aws_s3_object" "new_job_script" {
     bucket = aws_s3_bucket.iceberg_data_bucket.id
     key    = "scripts/glue_new_job.py"
     source = "../jobs/glue_new_job.py"
     etag   = filemd5("../jobs/glue_new_job.py")
   }
   
   resource "aws_glue_job" "new_job" {
     name     = "new-job-name"
     role_arn = aws_iam_role.glue_service_role.arn
     ...
   }
   ```

3. Update `jobs/README.md` with job documentation

4. Deploy:
   ```bash
   cd terraform
   terraform apply
   ```

### Adding a New Helper Script

1. Create script in `scripts/` folder:
   ```bash
   touch scripts/new_helper_script.py
   ```

2. Add documentation comment at top of file

3. Update this file (`FOLDER-STRUCTURE.md`) if significant

4. No deployment needed (runs locally)

### Adding New Documentation

1. Create markdown file in `docs/` folder:
   ```bash
   touch docs/new-guide.md
   ```

2. Follow markdown best practices

3. Link from main `README.md` if appropriate

---

## Best Practices

### 1. Keep Jobs and Scripts Separate

```
âœ… DO:
jobs/       â†’ Glue ETL jobs only
scripts/    â†’ Helper scripts only

âŒ DON'T:
scripts/    â†’ Mix Glue jobs and helper scripts
```

### 2. Document Everything

```
âœ… DO:
- Add README to each major folder
- Document job parameters
- Include usage examples

âŒ DON'T:
- Leave undocumented scripts
- Skip parameter descriptions
```

### 3. Use Consistent Naming

```
âœ… DO:
- Follow naming conventions
- Use descriptive names
- Be consistent across files

âŒ DON'T:
- Use generic names (script1.py)
- Mix naming styles
- Use abbreviations
```

### 4. Organize by Purpose

```
âœ… DO:
- Group related files
- Separate concerns
- Clear folder purposes

âŒ DON'T:
- Put everything in root
- Mix unrelated files
- Create deep nesting
```

---

## Quick Reference

### Where Does This Go?

| File Type | Folder | Example |
|-----------|--------|---------|
| Glue ETL job | `jobs/` | `glue_csv_to_iceberg.py` |
| Helper script | `scripts/` | `generate_sample_csv.py` |
| Terraform config | `terraform/` | `main.tf` |
| Documentation | `docs/` | `naming-conventions.md` |
| Sample data | `data/` | `sample.csv` |
| Root-level docs | `.` (root) | `ARCHITECTURE.md` |

### Common Tasks

| Task | Command |
|------|---------|
| Deploy infrastructure | `cd terraform && terraform apply` |
| Run ingestion job | `aws glue start-job-run --job-name csv-to-iceberg-ingestion` |
| Run views job | `aws glue start-job-run --job-name create-views-normal` |
| Generate sample data | `python scripts/generate_sample_csv_mdrm.py` |
| Setup Lake Formation | `python scripts/setup_lakeformation_complete.py` |
| Test views | `python scripts/test_views_in_spark.py` |

---

## Maintenance

### Regular Updates

- âœ… Keep documentation in sync with code
- âœ… Update README files when adding new files
- âœ… Review and clean up unused files
- âœ… Update this structure document as needed

### Version Control

- âœ… Commit related changes together
- âœ… Use meaningful commit messages
- âœ… Tag releases
- âœ… Keep .gitignore updated

---

**Last Updated**: February 11, 2026  
**Status**: Current âœ…
